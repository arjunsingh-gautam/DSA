# <span style="color:yellow">**Asymptotic Notation**</span>

## 1. Definition of Big O Notation

- **Big O notation** describes the **upper bound** (worst-case growth rate) of an algorithmâ€™s running time (or space usage) as the input size `n` grows.
- It ignores constants and low-order terms, focusing only on the **dominant factor** that dictates growth when `n â†’ âˆž`.

ðŸ‘‰ Formally:
If an algorithmâ€™s running time is `f(n)`, we say
**f(n) âˆˆ O(g(n))**
if there exist constants `c > 0` and `nâ‚€ > 0` such that:

[
f(n) \leq c \cdot g(n) \quad \text{for all } n \geq nâ‚€
]

---

## 2. How to Calculate Big O Mathematically

Steps:

1. **Express running time** as a function of input size `n`.
   Example: `f(n) = 3nÂ² + 5n + 10`
2. **Keep the dominant term** (highest growth rate).
   â†’ Here, `nÂ²` dominates.
3. **Drop constants**.
   â†’ `3nÂ²` becomes `nÂ²`.
4. Result: **O(nÂ²)**

---

## 3. Examples

### Example 1: Linear Search

```python
def linear_search(arr, target):
    for i in range(len(arr)):
        if arr[i] == target:
            return i
    return -1
```

- In worst case, we check all `n` elements.
- Time = `O(n)`

---

### Example 2: Nested Loop

```python
def pairs(arr):
    for i in range(len(arr)):
        for j in range(len(arr)):
            print(arr[i], arr[j])
```

- Outer loop runs `n` times.
- Inner loop runs `n` times per outer loop.
- Total = `n Ã— n = nÂ²` â†’ `O(nÂ²)`

---

### Example 3: Mixed terms

```python
def func(arr):
    for i in arr:          # O(n)
        print(i)
    for i in arr:          # O(n)
        for j in arr:      # O(nÂ²)
            print(i, j)
```

- First loop = O(n)
- Second = O(nÂ²)
- Total = O(nÂ² + n) â†’ O(nÂ²) (drop smaller terms)

---

## 4. What Info Big O Conveys

- **Rate of growth**: How quickly running time grows as input increases.
- **Scalability**: Whether algorithm is practical for large `n`.
- **Worst-case guarantee**: Ensures performance wonâ€™t exceed this bound.

---

## 5. Use Cases of Big O

- **Compare algorithms** (e.g., O(n log n) sort vs O(nÂ²) sort).
- **Predict performance** for large datasets.
- **Interview shorthand**: When asked about complexity, you answer in Big O.
- **System design**: Helps choose between approaches depending on input size.

---

## 6. Key Takeaways

- Big O **ignores constants** (O(2n) = O(n)).
- Focuses on **worst case**, but other notations (Î©, Î˜) handle best/average.
- Itâ€™s about **growth rates**, not actual time in seconds.

---

âœ… **Simple analogy:**
Big O is like describing how a car _accelerates_ with more passengers, not its actual speed at a moment. It shows the **trend**, not the exact number.

---

## 1. Definition of Î© (Omega) Notation

- **Î©-notation** describes the **asymptotic lower bound** of an algorithmâ€™s running time (or space).
- It tells us the **best-case growth rate**: the minimum time the algorithm will take for input size `n`.

ðŸ‘‰ Formally:
If an algorithmâ€™s runtime is `f(n)`, we say
**f(n) âˆˆ Î©(g(n))**
if there exist constants `c > 0` and `nâ‚€ > 0` such that:

[
f(n) \geq c \cdot g(n) \quad \text{for all } n \geq nâ‚€
]

---

## 2. How to Calculate Î© Mathematically

Steps:

1. Express running time `f(n)` as a function of input size.
2. Look for the **slowest growing dominant term** (best case).
3. Drop constants.
4. That gives the Î© bound.

---

## 3. Examples

### Example 1: Linear Search

```python
def linear_search(arr, target):
    for i in range(len(arr)):
        if arr[i] == target:
            return i
    return -1
```

- **Best case**: target is at index `0` â†’ just 1 comparison.

- So, `Î©(1)`

- **Worst case** (Big O): O(n)

---

### Example 2: Bubble Sort

```python
def bubble_sort(arr):
    for i in range(len(arr)):
        for j in range(len(arr)-i-1):
            if arr[j] > arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]
```

- Best case: array is already sorted â†’ inner swap never happens, but we still check each element once â†’ Î©(n).
- Worst case: O(nÂ²).

---

### Example 3: Simple Function

`f(n) = 4nÂ² + 3n + 10`

- For Î©, we take the **dominant term that always grows fastest** â†’ `nÂ²`.
- So, Î©(nÂ²).

---

## 4. What Info Î© Conveys

- **Lower bound guarantee**: At least this much time is needed, no matter what.
- **Best case** scenario of performance.
- **Efficiency floor**: Tells us how â€œfastâ€ the algorithm can possibly be.

---

## 5. Use Cases

- Useful to analyze whether an algorithm is **sometimes efficient**.
  (e.g., QuickSort has Î©(n log n), meaning best case is very good).
- Complements Big O:

  - Big O â†’ worst-case ceiling
  - Î© â†’ best-case floor

- Helps in **average-case analysis** (together with Î˜).

---

## 6. Quick Comparison

- **Big O (O)** â†’ at most this much time.
- **Omega (Î©)** â†’ at least this much time.
- **Theta (Î˜)** â†’ tightly bounded both above and below.

âœ… **Analogy:**
Think of running a marathon:

- **Î©** = minimum time you could ever finish (even if everything goes perfectly).
- **O** = maximum time (even if everything goes wrong).
- **Î˜** = your realistic average range.

---

## 1. Definition of Î˜ (Theta) Notation

- **Î˜-notation** describes the **asymptotically tight bound** of an algorithmâ€™s running time (or space).
- It means the algorithm grows **at least as fast** as some function _and_ **at most as fast** as that same function.

ðŸ‘‰ Formally:
If runtime is `f(n)`, we say
**f(n) âˆˆ Î˜(g(n))**
if there exist constants `câ‚, câ‚‚ > 0` and `nâ‚€ > 0` such that:

[
c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n) \quad \text{for all } n \geq nâ‚€
]

So, Î˜ means:

- `f(n)` is **sandwiched** between two multiples of `g(n)`.

---

## 2. How to Calculate Î˜ Mathematically

1. Write down runtime function `f(n)`.
2. Identify **dominant term**.
3. If both Big O (upper bound) and Omega (lower bound) are the same â†’ Thatâ€™s Î˜.

Example:
`f(n) = 3nÂ² + 5n + 20`

- O(nÂ²) (upper bound)
- Î©(nÂ²) (lower bound)
  â†’ So Î˜(nÂ²).

---

## 3. Examples

### Example 1: Linear Search

- Worst case: O(n)
- Best case: Î©(1)
- Since O â‰  Î© â†’ **not Î˜(n)**

---

### Example 2: Binary Search

- Worst case: O(log n)
- Best case: Î©(1)
- Since O â‰  Î© â†’ **not Î˜(log n)**

---

### Example 3: Merge Sort

- Worst case: O(n log n)
- Best case: Î©(n log n)
- Same upper & lower bounds â†’ **Î˜(n log n)**

---

## 4. What Î˜ Conveys

- **Tight bound**: Gives the _exact_ growth rate, ignoring constants.
- If an algorithm is Î˜(n log n), it means its time will always grow on the order of `n log n`, no better, no worse.
- More precise than just Big O or Î© alone.

---

## 5. Use Cases

- When analyzing algorithms in **textbooks, research, or interviews**, Î˜ is often preferred because it gives a **complete performance profile**.
- Good for comparing two algorithmsâ€™ _true asymptotic efficiency_.
- Helps detect if an algorithmâ€™s best, average, and worst cases are similar.

---

## 6. Quick Comparison

- **Big O (O)** = maximum time (ceiling).
- **Omega (Î©)** = minimum time (floor).
- **Theta (Î˜)** = exact growth rate (sandwiched).

âœ… **Analogy:**
Think of Î˜ as a **speedometer range**:
If you always drive between 50â€“60 km/h, we can tightly say: Î˜(55 km/h).
But if sometimes 10 km/h and sometimes 100 km/h, then Î˜ doesnâ€™t exist â€” only O and Î© separately.

---
