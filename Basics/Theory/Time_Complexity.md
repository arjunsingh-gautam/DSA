# <span style="color:yellow">**Time Complexity Analysis of Algorithm**</span>

## 1. **A Priori Analysis** (Before Implementation)

- **Definition**: Theoretical analysis of an algorithmâ€™s efficiency **before coding or running it**.
- **Basis**: Mathematical reasoning, logical steps, Big-O complexity.
- **Focus**: Time complexity, space complexity, input size dependency.
- **Goal**: Predict whether the algorithm will work within constraints.

### âš™ï¸ Methodology / Elements

1. Express number of operations in terms of input size `n`.
2. Ignore machine/hardware factors (idealized model of computation).
3. Classify growth rate: O(1), O(log n), O(n), O(nÂ²), etc.
4. Compare alternative algorithms theoretically.

### âœ… Example:

**Finding the maximum element in an array of size n**

- Step 1: Compare each element once â†’ `n-1` comparisons.
- Step 2: Time complexity = O(n).
- Step 3: Space complexity = O(1).

ğŸ‘‰ Even before coding, you know scanning once is the most efficient possible method.

---

## 2. **A Posteriori Analysis** (After Implementation)

- **Definition**: Experimental analysis of an algorithm **after coding and executing it**.
- **Basis**: Practical performance on real hardware.
- **Focus**: Actual running time, memory usage, scalability on large inputs.
- **Goal**: Verify or refine theoretical predictions.

### âš™ï¸ Methodology / Elements

1. Implement the algorithm in a programming language.
2. Run with test inputs (small, medium, large).
3. Measure CPU time, memory usage, I/O operations.
4. Compare results across machines, compilers, input distributions.

### âœ… Example:

**Finding the maximum element in an array of size 10â·**

- Implement algorithm in Python.
- Run with large input on your machine.
- Measure actual runtime (say ~0.2 seconds).
- Check memory footprint.
- Compare with another implementation (e.g., using NumPyâ€™s `max()`) â€” maybe it runs in ~0.05 seconds because of optimized C code.

ğŸ‘‰ Confirms theoretical O(n), but gives _practical insight_ into efficiency differences.

---

## ğŸ”„ Comparison: Which is More Important?

- **A Priori (theory)** â†’ helps you **design** algorithms, compare approaches quickly, and reason about scalability.
- **A Posteriori (experiment)** â†’ helps you **validate** actual performance, considering hardware, language, compiler, caching, etc.

âš–ï¸ **In practice**:

- For **interviews & exams** â†’ _A Priori_ is more important (they expect you to analyze Big-O).
- For **real-world system design** â†’ _A Posteriori_ matters equally (because theory doesnâ€™t capture CPU caches, memory hierarchy, parallelism, etc.).

---

## ğŸ”‘ Best Practice When Designing Algorithms

1. Do **A Priori analysis** first (time/space complexity).
2. If multiple algorithms are possible, compare theoretical bounds.
3. Then **implement and test** with A Posteriori analysis to confirm and optimize.
4. Always watch for mismatches (sometimes an O(n log n) algorithm beats an O(n) one in practice because of constant factors).

---

âœ… Quick Recap:

- **A Priori** â†’ before coding, mathematical analysis.
- **A Posteriori** â†’ after coding, experimental analysis.
- Both are important, but for problem-solving/design: **start with A Priori**.

---

## â³ **Time Complexity**

ğŸ‘‰ Definition: Number of **basic operations** an algorithm performs as a function of input size `n`.
_(Itâ€™s about **growth rate**, not actual seconds.)_

### ğŸ”¹ Characteristics

- **Input-size dependent** (not machine dependent).
- Expressed with Big-O (worst case), sometimes Î˜ (tight bound), or Î© (best case).
- Looks at operations like comparisons, assignments, arithmetic.

### ğŸ”¹ Causes

- Loops (linear, nested, logarithmic).
- Recursion depth.
- Function calls and repeated computations.
- Data access patterns (array vs linked list).

### ğŸ”¹ Effect

- High time complexity (O(nÂ²), O(2â¿)) â†’ slow for large inputs.
- Determines whether algorithm is _practically usable_.
- Can bottleneck overall system performance.

---

# ğŸ’¾ **Space Complexity**

ğŸ‘‰ Definition: Amount of **memory/storage** required by an algorithm to execute.

### ğŸ”¹ Characteristics

- Includes:

  1. Fixed part â†’ code, constants, simple variables.
  2. Variable part â†’ dynamic allocations, recursion stack, input data.

- Usually expressed as a function of input size `n`.

### ğŸ”¹ Causes

- Extra data structures (arrays, hash maps, trees).
- Recursion (stack frames).
- Caching / memoization.
- Multiple copies of data.

### ğŸ”¹ Effect

- High space usage â†’ memory overflow, cache misses, paging â†’ slower execution.
- Limits algorithmâ€™s scalability (canâ€™t run on devices with low memory).

---

## âš–ï¸ **Trade-off Between Time & Space Complexity**

- Often, reducing time needs _more space_, and vice versa.

### Examples:

1. **Memoization in DP**

   - Saves subproblem results in memory â†’ improves time (from exponential â†’ polynomial) at the cost of extra space.

2. **Hashing vs Sorting**

   - Hash table lookups: O(1) time but O(n) extra space.
   - Sorting with no extra space: O(n log n) time but O(1) space.

3. **Recursion vs Iteration**

   - Recursion is elegant, but uses stack space.
   - Iteration uses less memory but can be harder to design.

ğŸ‘‰ **Which is more important?**

- **Time complexity** is _usually prioritized_ (especially in interviews).
- **Space** matters when working on memory-constrained systems (IoT, embedded, mobile apps).
- In modern big-data systems, _both matter equally_ (think of algorithms that run on TBs of data â€” memory is critical).

---

## ğŸ”„ **Correlation Between Time and Space Complexity**

- **Direct correlation**: More space can reduce time (e.g., precomputed tables).
- **Inverse correlation**: Optimizing space (in-place algorithms) can increase runtime (more recomputation).
- **Independent**: Sometimes one can improve both (better algorithm design).

---

## âœ… Recap

- **Time Complexity** â†’ speed of execution.
- **Space Complexity** â†’ memory used.
- **Trade-off** â†’ faster = more memory, less memory = slower.
- **Correlation** â†’ often inversely related, but depends on the problem.

---

## **What is â€œgrowth rateâ€?**

**Growth rate** = how the _number of elementary operations_ grows as input size `n` grows.
We model it with a function like `T(n)` (time) or `S(n)` (space) and describe its **asymptotic** behavior (Big-O, Î˜, Î©), **ignoring constants and low-order terms**.

- Examples of growth rates:

  - `Î˜(1)` constant
  - `Î˜(log n)` logarithmic (binary search)
  - `Î˜(n)` linear (single pass)
  - `Î˜(n log n)` (merge/quick sort, average)
  - `Î˜(nÂ²)` quadratic (naive double loops)

This tells you **scalability**: which algorithm will remain feasible as `n` becomes huge.

---

## Why growth rate â‰  actual execution time

**Wall-clock time** depends on many real-world factors that asymptotic analysis deliberately ignores:

1. **Constant factors & lower-order terms**

   - Real time â‰ˆ `c Â· T(n) + overhead`.
   - Two `Î˜(n)` algorithms can differ by 10Ã— due to constants.

2. **Language / runtime**

   - Python loops have higher per-iteration overhead than C loops.
   - Built-ins or NumPy (C-optimized) can beat â€œbetterâ€ Pythonic complexities for modest `n`.

3. **Hardware & system effects**

   - CPU cache, memory bandwidth, branch prediction, vectorization, disk I/O.

4. **Input distribution & cases**

   - Quicksort average `Î˜(n log n)`, but worst `Î˜(nÂ²)`; nearly-sorted inputs favor Timsort.

5. **Implementation details**

   - Data structure choices, copy vs in-place, recursion depth, allocation patterns.

Bottom line: **growth rate predicts trend; time measures reality.** Use both.

---

## Three concrete examples

### 1) Quadratic vs n log n (crossover with constants)

Let:

- **A (selection sort)**: `T_A(n) = 0.5 Â· nÂ²` â€œopsâ€ (simple inner loop, tiny constant)
- **B (quicksort)**: `T_B(n) = 5 Â· n Â· logâ‚‚ n + 1000` (faster growth rate but some overhead)

Check two `n`:

- `n = 50`

  - `logâ‚‚ 50` â‰ˆ 5.64
  - `T_A(50) = 0.5 Â· 2500 = 1250`
  - `T_B(50) = 5 Â· 50 Â· 5.64 + 1000 = 1410 + 1000 = 2410`
    ğŸ‘‰ Here, the **quadratic** algorithm is actually faster due to overheads.

- `n = 100`

  - `logâ‚‚ 100` â‰ˆ 6.64
  - `T_A(100) = 0.5 Â· 10000 = 5000`
  - `T_B(100) = 5 Â· 100 Â· 6.64 + 1000 = 3320 + 1000 = 4320`
    ğŸ‘‰ Now **n log n** wins.
    As `n` grows further, `nÂ²` explodes, so B dominates despite overhead.

**Lesson:** Growth rate says who wins _eventually_; constants decide _from which n_.

---

### 2) Same Big-O, different constants (pure Python vs built-in)

- **Sum with a Python loop**: `Î˜(n)` but high per-iteration overhead in Python.
- **`sum(arr)` (C fast path)**: also `Î˜(n)`, but a much smaller constant factor.
  In practice, `sum(arr)` is **much faster**, even though both are `Î˜(n)`.

**Lesson:** Big-O canâ€™t distinguish constant-factor wins that matter a lot in Python.

---

### 3) Spaceâ€“time trade (hash set vs sorted list)

- **Membership check via set**: build set once `Î˜(n)` time and `Î˜(n)` **extra space**; each query `Î˜(1)` average.
- **Binary search on sorted list**: `Î˜(n log n)` to sort (or `Î˜(log n)` per query if already sorted), **O(1) extra space**.
  If memory is tight, you might prefer the list approach even if queries are slower.

**Lesson:** Practical design balances _time_ vs _space_ vs _constraints_.

---

## How to use both in algorithm design

1. **A priori (theory):** pick an algorithm with the best **growth rate** that meets constraints.
2. **A posteriori (experiment):** implement and **measure** on your target platform and realistic inputs.
3. **Decide with constraints:** If `n` is always â‰¤ 10â´ and memory is scarce, a theoretically worse algorithm may be the right choice.

---

## ğŸ”¹ What is Asymptotic Notation?

Itâ€™s a **mathematical way to describe the behavior of an algorithm as input size â†’ âˆ** (gets very large).

Instead of measuring _exact_ operations or seconds, we describe the **trend** of growth.

ğŸ‘‰ Itâ€™s like saying: _â€œDonâ€™t sweat the small details, just tell me how fast it blows up when `n` is huge.â€_

---

## ğŸ”¹ The Principle Behind It

1. **Ignore constants & lower-order terms**

   - We only keep the â€œdominatingâ€ term that matters most for very large `n`.
   - Example: `5nÂ² + 10n + 7` â†’ growth is basically `nÂ²`.

2. **Abstract away machine details**

   - Doesnâ€™t matter if you use Python or C or a faster CPU; growth shape stays the same.

3. **Classify algorithms into families**

   - Linear, quadratic, logarithmic, exponential, etc.

This lets us compare algorithms fairly, at a high level.

---

## ğŸ”¹ The Three Core Notations

1. **Big-O (O):** _Upper bound_ â†’ worst-case growth.

   - Example: `O(nÂ²)` means it never grows faster than quadratic (ignoring constants).
   - Used to guarantee efficiency.

2. **Big-Î© (Omega):** _Lower bound_ â†’ best-case growth.

   - Example: `Î©(n)` means it will take at least linear time in some case.

3. **Big-Î˜ (Theta):** _Tight bound_ â†’ exact growth rate (both upper and lower).

   - Example: `Î˜(n log n)` means itâ€™s always roughly proportional to `n log n`.

ğŸ‘‰ Think of it like this:

- Big-O = â€œat most this fastâ€ (pessimistic parent).
- Big-Î© = â€œat least this fastâ€ (optimistic parent).
- Big-Î˜ = â€œexactly this fastâ€ (realistic parent).

---

## ğŸ”¹ Asymptotic Analysis of Time Complexity

We count how the **number of steps** scales with input size `n`.

Examples:

- **Linear Search**

  - Checks elements one by one.
  - Worst case: check all `n`.
  - `O(n)` time.

- **Binary Search**

  - Halves the search range each time.
  - After ~`logâ‚‚ n` steps, done.
  - `O(log n)` time.

- **Bubble Sort**

  - Nested loops over `n` elements.
  - Worst case: ~`nÂ²` comparisons.
  - `O(nÂ²)` time.

ğŸ‘‰ Asymptotic **time** complexity tells you:
â€œ**How execution time grows with bigger inputs.**â€

---

## ğŸ”¹ Asymptotic Analysis of Space Complexity

We count how **extra memory** (beyond input) scales with input size `n`.

Examples:

- **Recursive Fibonacci**

  - Call stack depth = `n`.
  - `O(n)` space.

- **Merge Sort**

  - Needs extra arrays to merge results.
  - `O(n)` space.

- **Binary Search** (iterative)

  - Just a few pointers, no big memory.
  - `O(1)` space.

ğŸ‘‰ Asymptotic **space** complexity tells you:
â€œ**How memory requirements grow with bigger inputs.**â€

---

## ğŸ”¹ In Simple Terms

- **Time complexity** â†’ How long will it take as inputs grow?
- **Space complexity** â†’ How much memory will it eat as inputs grow?
- **Asymptotic notation** â†’ Ignore machine speed; just focus on _the shape of growth_.

---

## ğŸ”¹ Example Walkthrough

Suppose we want to **find if a number exists in a sorted list of `n` elements**.

1. **Linear Search**

   - Time: `O(n)` (check one by one).
   - Space: `O(1)` (just one loop variable).

2. **Binary Search**

   - Time: `O(log n)` (cut the list in half each step).
   - Space:

     - Iterative: `O(1)` (a few pointers).
     - Recursive: `O(log n)` (call stack).

ğŸ‘‰ Both are _correct_, but asymptotic notation shows:
Binary search scales way better when `n` is huge.

---

### ğŸ”¹ 1. What does â€œtime complexity of a loopâ€ mean?

- **Definition**: Time complexity is a measure of **how the number of operations grows** with the size of input.
- For loops, the complexity depends on **how many times the loop body executes**.

ğŸ‘‰ So yes, at a high level: _the time complexity of a loop is directly proportional to the number of iterations_.

But thereâ€™s nuance: the number of iterations may depend on:

- constants (fixed number of times),
- input size (`n`),
- function of input size (like `n^2`, `log n`, etc.).

---

### ğŸ”¹ 2. Simple Examples

#### Example 1: Constant loop

```python
for i in range(10):   # runs 10 times
    print(i)
```

- Number of iterations = 10 (constant, independent of input size).
- **Time complexity = O(1)** (constant time).
  Why? Because no matter how large the input is, this loop always executes 10 times.

---

#### Example 2: Linear loop

```python
n = 100
for i in range(n):   # runs n times
    print(i)
```

- Number of iterations = `n`.
- **Time complexity = O(n)**.
  Why? As `n` grows, the loop body runs more times linearly.

---

#### Example 3: Nested loops

```python
n = 5
for i in range(n):
    for j in range(n):
        print(i, j)
```

- Outer loop: runs `n` times.
- Inner loop: runs `n` times **for each outer iteration**.
  ğŸ‘‰ Total iterations = `n * n = n^2`.
- **Time complexity = O(nÂ²)**.

---

#### Example 4: Logarithmic loop

```python
n = 16
i = 1
while i < n:
    print(i)
    i = i * 2
```

- Loop runs while `i < n`, doubling `i` each time: `1, 2, 4, 8, â€¦`.
- Number of iterations â‰ˆ `logâ‚‚(n)`.
- **Time complexity = O(log n)**.

---

### ğŸ”¹ 3. Key Principle

ğŸ‘‰ **Time complexity of a loop = number of iterations Ã— complexity of work done inside the loop.**

- If inside body is `O(1)` â†’ overall complexity is based only on iterations.
- If inside body has another loop or function â†’ multiply complexities.

---

### ğŸ”¹ 4. Tradeoff Examples

- **Loop 1 (linear)**: O(n)
- **Loop 2 (quadratic)**: O(nÂ²)
- **Loop 3 (logarithmic)**: O(log n)
  Even though all are "loops," their growth rate differs drastically.

---

âœ… **Simple rule of thumb for interviews**:

> "Count how many times each statement inside runs relative to input size."

---

## 1. What Are Complexity Classes?

- A **complexity class** groups algorithms according to **how fast their running time (or space usage) grows** as input size `n` increases.
- They give us a _scale_ to compare algorithms abstractly, ignoring machine speed, coding style, or small constants.

ğŸ‘‰ **Importance:**

- Helps us **predict scalability**: Will the algorithm still work when `n = 10^6` or `10^12`?
- Guides us to choose the **best algorithm for the job**.
- Interviewers test this because it shows your **intuition for efficiency**.

---

## 2. Common Complexity Classes

Letâ€™s line them up (slowest growth â†’ fastest growth):

### **Constant: O(1)**

- Execution doesnâ€™t depend on input size.
- Ex: Accessing an array element `arr[i]`.
- **Use:** Lookups, hashing.

---

### **Logarithmic: O(log n)**

- Input is reduced significantly at each step (like binary splitting).
- Ex: Binary search.
- **Use:** Searching in sorted data, tree operations.

---

### **Polylogarithmic: O((log n)^k)**

- Slightly slower than `O(log n)` but much faster than linear.
- Ex: advanced divide-and-conquer, some data structures.
- Rare in practice but shows up in theoretical bounds.

---

### **Linear: O(n)**

- Work grows directly with input size.
- Ex: Traversing an array, finding min/max.
- **Use:** Scanning, simple algorithms.

---

### **n log n: O(n log n)**

- Combination of linear work with logarithmic splitting.
- Ex: Merge sort, Quick sort (average).
- **Use:** Efficient sorting, divide & conquer problems.
- âœ… **Most â€œpracticalâ€ fastest class for general-purpose algorithms.**

---

### **Quadratic: O(nÂ²)**

- Nested loops over input.
- Ex: Bubble sort, checking all pairs.
- **Use:** Small input sizes, brute force comparisons.

---

### **Polynomial: O(n^k), k > 2**

- Triple loops or more.
- Ex: Floyd-Warshall algorithm (O(nÂ³)).
- **Use:** Some dynamic programming, matrix multiplication.

---

### **Exponential: O(2^n), O(3^n), â€¦**

- Growth explodes; practical only for very small inputs.
- Ex: Subset generation, naive recursion for Fibonacci.
- **Use:** Brute-force search in NP-complete problems.

---

### **Factorial: O(n!)**

- Extremely large growth.
- Ex: Solving Traveling Salesman by brute force.
- **Use:** Only in brute-force combinatorial search.

---

## 3. Intermediate Classes You Mentioned

These come from **theoretical computer science** but theyâ€™re worth knowing:

### **(log n)^k (polylogarithmic)**

- Grows slower than linear (`O(n)`).
- Example: Some graph algorithms or number theory optimizations.
- Importance: Shows thereâ€™s a middle ground between `O(log n)` and `O(n)`.

---

### **(log n)^(log n)**

- This is larger than `n` but smaller than, say, `n^2`.
- Why? Because `log n * log n` grows slower than linear times linear.
- Appears in advanced analysis of recursive algorithms and certain combinatorial problems.

---

### **Sub-linear: o(n)**

- Less than linear.
- Ex: Binary search (O(log n)), hash lookups (O(1) expected).
- **Importance:** Shows algorithms can solve problems without â€œtouching every element.â€

---

### **Super-polynomial but sub-exponential (e.g., n^(log n))**

- Between polynomial and exponential.
- Appears in complex graph algorithms, cryptographic hardness proofs.
- **Importance:** Distinguishes â€œhard but not impossibleâ€ problems.

---

## 4. What Information Each Class Conveys

- **Scalability**: Tells you whether algorithm is usable at large input sizes.
- **Design choice**: Helps choose the best approach (sorting with `O(n log n)` vs `O(nÂ²)`).
- **Theoretical boundary**: Some classes (like exponential) indicate that _no practical solution_ exists for large `n`.

---

## 5. Trade-offs and Uses

- If input sizes are **small** â†’ O(nÂ²) may be acceptable.
- If input sizes are **large** â†’ need O(n log n) or better.
- Sometimes **space complexity** can reduce time (using extra memory for hashing).

---

## 6. Visual Growth Intuition

Imagine `n = 1000`:

- O(1) â†’ 1 step
- O(log n) â†’ ~10 steps
- O(n) â†’ 1000 steps
- O(n log n) â†’ ~10,000 steps
- O(nÂ²) â†’ 1,000,000 steps
- O(2^n) â†’ ğŸ”¥ unimaginable (2^1000 is astronomically large)

---

âœ… **Summary in plain words:**
Complexity classes are like a **map of difficulty levels** for algorithms.
They tell us: _If input grows 10Ã—, how much slower will the algorithm get?_

---
